# DIAYN-PyTorch

## Multiprocessing Training

`DIAYN`的base选择的是`SAC`，这个一个`off-policy`的算法，在fork的版本中，作者每交互一步，网络权重更新一次，那么5000个episode，每个episode都按最大步长1000来算，更新次数和交互次数都为500w，经验池固定容量10w

在我尝试修改为多进程训练时，遇到的棘手问题，就是如何控制更新频率，因为网络权重要全局同步（虽说可以设置`target network`，以及一个同步频率参数来控制，但如上所说，原始实现是每次交互都是最新的权重），假如我也保持原来的设置，会导致网络权重频繁保存和加载，并且是在多进程之间，这样的通信消耗非常大。退而求其次，我选择每有一个进程返回一批轨迹数据时，根据轨迹的步长，除以进程数，得到该轮更新次数，一次完整的epidose，会获取进程数个批数据，因此，相当于更新了这些轨迹数据的平均步长次数。

有意思的是，`HalfCheetah-v3`这样的环境，不容易done，主要靠`max_step`来限制一条轨迹的长度，这种条件下，我的多进程训练网络更新次数与单进程训练会一致，区别在于多进程训练与环境交互次数是单进程的 **N**（进程数）倍。

实际对比的单个episode的训练速度不尽如人意，如上所述，这种改进并没有在更新频率上有提升，主要区别在于经验池中的数据。假如因为数据的新旧交替更快，能使得训练收敛速度加快，这样的多进程训练从宏观来说，也是有意义的。

另外一些细节，在CUDA中的参数如果需要进程间共享，需要启用"spawn"
```pythoin
torch.multiprocessing.set_start_method('spawn')
```

这样会使得所有网络始终运行在GPU上（推理与训练），对显存有一定要求（HalfCheetah-V3测试环境，向量输入）
* **4** processes will used **9GB** memory on GPU
* **10** processes will used **18.5GB** memory on GPU

当然，我们也可以将推理部分放在CPU上进行，在网络简单时，这样不会损失太多效率，同时能节省大量显存。
* used **1.5GB** memory on GPU

一般我们默认要调整最大允许打开文件数
```bash
ulimit -n 65536
```
## How to use it

这里的config设置和单进程一样，多了进程数的参数，以及选择采样交互是在"CPU"还是"GPU"上进行，别的都保持一致，建议直接在config中修改，然后直接运行

```bash
python main_multi.py
```

可视化需要注意model的路径和video保存的路径，需要在`view_play.py`中修改
```bash
python view_play.py
```
### 灵光一现

说着说着，突然想起来一些优秀的开源框架，在实现多进程的训练时，会采用并行环境的方法，相当于每次交互的输入不再是`[1,n_state]`，而是`[n_env,n_state]`，一次前向推理完成多个环境的需求，此时多进程只包含环境交互，队列传递的是状态、动作、奖励，模型始终运行在一个进程上，全局只用维护一个agent。很容易想到，这样处理的一个难点在于，返回的动作怎么分配给各自进程中的环境。有时间可以学习以下这种实现方式。

## 本地实验（09.15）

对比多进程训练与之前单进程训练的效果，其中多进程训练按推理设备分为**GPU**和**CPU**两组。

实测下来，多进程版本在相同的更新次数下，比单进程慢2~2.5倍，多进程**GPU**组和**CPU**组在速度上保持一个量级，**CPU**组略快（可能是占用了**CPU**资源，导致**GPU**组慢了点）。多进程的优势是交互多，探索到的最大奖励的值会更大，且`logq_z`的曲线更早收敛（是好是坏？）可视化了600step和1.4kstep的skills，发现对应的skill会有略微变化，极个别变化明显，但这种变化和该skill得到的episode reward没有相关性。

## 怎么评估无监督学习到的技能

这个是有点主观的成分在里面，在没有详细浏览paper中是实验之前，我认为直接对技能可视化是最直接也最另人信服的方式。但这样做的成本可能有点大，同时这种带有主观偏见的判断难以区分得更加细致。

paper中提到了一些实验以及思想：
  
* 在一个有限的状态空间内，如果不同技能分布在不同的区域，总体上尽可能涵盖全部的状态空间，那么说这些技能是具有多样性且可区分的，这是好的表现。
* 随着训练次数的增加，**不同技能所对应的奖励，在分布上是多样化的，并不是追求所有的技能都获得高奖励**。
* 对比VIC算法，由于DIAYN固定了技能数，所以随着训练的进行，policy学到的技能数不会减少，而VIC会有较大幅度的减少。
* 不同的技能有利于完成下游任务，在具体实践中，我们可以选择最接近任务目标的技能，以此为初始化策略，能加快训练速度。
* 即便是在没有任务奖励的条件下学习，当用特定任务奖励去评估时，仍能发现部分技能可以获得不错的reward。
* 技能学习应该对随机种子具有一定的鲁棒性。