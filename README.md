# DIAYN-PyTorch

## Multiprocessing Training

`DIAYN`的base选择的是`SAC`，这个一个`off-policy`的算法，在fork的版本中，作者每交互一步，网络权重更新一次，那么5000个episode，每个episode都按最大步长1000来算，更新次数和交互次数都为500w，经验池固定容量10w

在我尝试修改为多进程训练时，遇到的棘手问题，就是如何控制更新频率，因为网络权重要全局同步（虽说可以设置`target network`，以及一个同步频率参数来控制，但如上所说，原始实现是每次交互都是最新的权重），假如我也保持原来的设置，会导致网络权重频繁保存和加载，并且是在多进程之间，这样的通信消耗非常大。退而求其次，我选择每有一个进程返回一批轨迹数据时，根据轨迹的步长，除以进程数，得到该轮更新次数，一次完整的epidose，会获取进程数个批数据，因此，相当于更新了这些轨迹数据的平均步长次数。

有意思的是，`HalfCheetah-v3`这样的环境，不容易done，主要靠`max_step`来限制一条轨迹的长度，这种条件下，我的多进程训练网络更新次数与单进程训练会一致，区别在于多进程训练与环境交互次数是单进程的 **N**（进程数）倍。

实际对比的单个episode的训练速度不尽如人意，如上所述，这种改进并没有在更新频率上有提升，主要区别在于经验池中的数据。假如因为数据的新旧交替更快，能使得训练收敛速度加快，这样的多进程训练从宏观来说，也是有意义的。

另外一些细节，在CUDA中的参数如果需要进程间共享，需要启用"spawn"
```pythoin
torch.multiprocessing.set_start_method('spawn')
```

一般我们默认要调整最大允许打开文件数
```bash
ulimit -n 65536
```

这样会使得所有网络始终运行在GPU上（推理与训练），对显存有一定要求（HalfCheetah-V3测试环境，向量输入）
* **4** processes will used **9GB** memory on GPU
* **10** processes will used **18.5GB** memory on GPU

当然，我们也可以将推理部分放在CPU上进行，在网络简单时，这样不会损失太多效率，同时能节省大量显存。
* used **1.5GB** memory on GPU

## How to use it

这里的config设置和单进程一样，多了进程数的参数，以及选择采样交互是在"CPU"还是"GPU"上进行，别的都保持一致，建议直接在config中修改，然后直接运行

```bash
python main_multi.py
```

可视化需要注意model的路径和video保存的路径，需要在`view_play.py`中修改
```bash
python view_play.py
```
### 灵光一现

说着说着，突然想起来一些优秀的开源框架，在实现多进程的训练时，会采用并行环境的方法，相当于每次交互的输入不再是`[1,n_state]`，而是`[n_env,n_state]`，一次前向推理完成多个环境的需求，此时多进程只包含环境交互，队列传递的是状态、动作、奖励，模型始终运行在一个进程上，全局只用维护一个agent。很容易想到，这样处理的一个难点在于，返回的动作怎么分配给各自进程中的环境。有时间可以学习以下这种实现方式。

## 本地实验（09.15）

对比多进程训练与之前单进程训练的效果，其中多进程训练按推理设备分为GPU和CPU两组。